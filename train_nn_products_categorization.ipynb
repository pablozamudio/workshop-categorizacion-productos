{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop_categorizacion-empty.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablozamudio/workshop-categorizacion-productos/blob/master/train_nn_products_categorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU2-6XDJugy9",
        "colab_type": "text"
      },
      "source": [
        "# Workshop Categorizacion \n",
        "Items categorization with NLP, Keras, Tensorflow, Pandas and Numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_TlFUAJuVFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "from collections import Counter\n",
        "import random\n",
        "from IPython.display import Image\n",
        "# https://github.com/sepandhaghighi/pycm\n",
        "!pip install pycm==1.8\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12,9))\n",
        "\n",
        "import pandas as pd # data preprocessing\n",
        "import numpy as np # linear algebra\n",
        "\n",
        "from tensorflow.python.keras.preprocessing import sequence\n",
        "from tensorflow.python.keras.preprocessing import text\n",
        "\n",
        "pd.set_option('display.max_columns', None)  # or 1000\n",
        "pd.set_option('display.max_rows', None)  # or 1000\n",
        "pd.set_option('display.max_colwidth', -1)  # or 199"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT8HoVgQvCSL",
        "colab_type": "text"
      },
      "source": [
        "# Gather the Data\n",
        "\n",
        "Download the zipped dataset of items from Google Drive (using the shareable link) and uncompress it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9apfpuWGuhGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "\n",
        "download_file_from_google_drive('1kSPUjkf7gkB77UVTNnIW_cmGIjsa7GSh', 'arq.png')                \n",
        "download_file_from_google_drive('1Z0ikfy9XseV2o5WtitKtfN22mGgP-MZs', 'vec.png')\n",
        "\n",
        "download_file_from_google_drive('1Ql0az0GAC1_yHpHsAFvspKt1HZGMZOOa', 'cellphones.zip')\n",
        "!unzip 'cellphones.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFQE2kyovide",
        "colab_type": "text"
      },
      "source": [
        "Here is the architecture of the Neural Network that we are going to train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5IY2gbevas2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('arq.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiI1NwEMvqTa",
        "colab_type": "text"
      },
      "source": [
        "## Load the dataset using pandas\n",
        "The dataset is a csv **tab separated**\n",
        "\n",
        "See: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.htm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt_KRTWHvlAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1 LoC aprox.\n",
        "def read_csv(path='cellphones.csv'):\n",
        "  #######################################\n",
        "  ########### Your code here ############\n",
        "  #######################################\n",
        "  dataset = \n",
        "  return dataset\n",
        "\n",
        "df = read_csv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVC63MXdwQOb",
        "colab_type": "text"
      },
      "source": [
        "# Explore the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z19Djbs6tSr",
        "colab_type": "text"
      },
      "source": [
        "Print some lines to see what the dataset looks like"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vajLlJ-fwMwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwwWtFZPwtg4",
        "colab_type": "text"
      },
      "source": [
        "Find how many classes we have in the dataset. \n",
        "\n",
        "This classes will be the output of the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8PkZ-QkwYrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1 LoC aprox.\n",
        "# Tip use either Pandas .unique() or .nunique() functions\n",
        "def get_num_classes(df):\n",
        "  #######################################\n",
        "  ########### Your code here ############\n",
        "  #######################################\n",
        "  output_length = \n",
        "  return output_length\n",
        "\n",
        "num_classes = get_num_classes(df)\n",
        "assert num_classes == 25\n",
        "print(\"Output Lenght:\", num_classes)\n",
        "print(\"# Training examples:\", len(df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6wCQg2oxOBL",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the # of items per class\n",
        "\n",
        "Bar plot the number of items per class.\n",
        "\n",
        "You could do:\n",
        " 1. groupby('domain_id') \n",
        " 2. count() \n",
        " 3. sort_values()\n",
        " 4. plot.bar(...)\n",
        "\n",
        "or just use:\n",
        " 1. value_counts()\n",
        " 2. plot.bar(...)\n",
        "\n",
        "See: \n",
        "\n",
        "Pandas plot bar https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.bar.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcUCoj5xxlds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From 1 to 4 LoC aprox. \n",
        "def plot_items_per_class(df):\n",
        "  #######################################\n",
        "  ########### Your code here ############\n",
        "  #######################################\n",
        "  \n",
        "\n",
        "plot_items_per_class(df)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZn1Yzje32DJ",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the class distribution\n",
        "\n",
        "Plot the histogram - A histogram is a representation of the distribution of data\n",
        "\n",
        "See: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.hist.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnMZK8YS45xJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From 1 to 4 LoC aprox. \n",
        "# Almost identical to previos solution\n",
        "def plot_distribution(df):\n",
        "  #######################################\n",
        "  ########### Your code here ############\n",
        "  #######################################\n",
        "  \n",
        "\n",
        "plot_distribution(df)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5FG1CgW6IiM",
        "colab_type": "text"
      },
      "source": [
        "# Prepare the data for training\n",
        "Here is an image showing what we are to do with the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXS0_AF05MQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display(Image('vec.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svzIj9yN6nOY",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization and Vectorization\n",
        "\n",
        "Tokenization: Divide the texts into words or smaller sub-texts (tokens). This determines the “vocabulary” of the dataset.\n",
        "\n",
        "Vectorization: Define a good numerical measure to characterize these texts.\n",
        "\n",
        "Steps:\n",
        "1. Fit a Keras Tokenizer using the text corpus (all the titles in the dataset)\n",
        "2. Build a dictionary of indexes -> tokens\n",
        "3. Create a function that parse the text to a sequence of decimals representing each token (step 2 in the image presented)\n",
        "4. Vectorize the text using different representations or techniques (tfidf, bow, binary, etc)\n",
        "5. Vectorize the labels (outputs of the NN)\n",
        "6. Build a dictionary of indexes -> labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-imRqfH2_e6W",
        "colab_type": "text"
      },
      "source": [
        "### Fit a Keras Tokenizer\n",
        "\n",
        "**From** https://keras.io/preprocessing/text/:\n",
        "\n",
        "This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-XhMWcd6ZDP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 LoC aprox.\n",
        "def fit_keras_tokenizer(corpus, num_features):\n",
        "  #######################################\n",
        "  ########### Your code here ############\n",
        "  #######################################\n",
        "  tokenizer = \n",
        "  # Your code here (remember to call tokenizer.fit_on_texts)\n",
        "  return tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RblgcsIY_vo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "tic = time.time()\n",
        "num_features=5000\n",
        "\n",
        "tokenizer = fit_keras_tokenizer(df['title'].values, num_features)\n",
        "\n",
        "toc = time.time()    \n",
        "print(\"Time to fit tokenizer: \" + str(1000*(toc-tic)) + \" ms\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bjLJ3CFAEKs",
        "colab_type": "text"
      },
      "source": [
        "### Print tokenizer vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnbqWClo_13S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbJk_CAgEjzw",
        "colab_type": "text"
      },
      "source": [
        "Visualize what the vectorizer is actually doing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ypr9PK5B82b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_vectorized_title(title, tokenizer, num_features, mode='count'):\n",
        "  print(\"------------------\")\n",
        "  print(\"Title:\", title)\n",
        "  print(\"------------------\")\n",
        "  print(\"Vectorizing title using %s mode\" % mode)\n",
        "  print(\"------------------\")\n",
        "\n",
        "  vectorized_title = tokenizer.texts_to_matrix([title], mode)\n",
        "  assert vectorized_title.shape == (1, num_features)\n",
        "  \n",
        "  for tkn in title.split(' '):\n",
        "    tkn_idx = tokenizer.word_index[tkn]\n",
        "    print(tkn ,vectorized_title[0][tkn_idx])\n",
        "  return vectorized_title\n",
        "\n",
        "# Try changing mode to 'count' and 'tfidf'\n",
        "title = \"tablet de tablet samsung con para pantalla tablet\"\n",
        "vt = print_vectorized_title(title, tokenizer, num_features, 'binary')\n",
        "print(\"\")\n",
        "print(\"Vectorized title Vector:\", vt)\n",
        "print(\"Vectorized title Shape:\", vt.shape)\n",
        "print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cmrezYdCt5N",
        "colab_type": "text"
      },
      "source": [
        "### Vectorize the text and labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZ0CDIdODapx",
        "colab_type": "text"
      },
      "source": [
        "This function append a new column called \"label\" with an integer that uniquely identify that domain_id.\n",
        "See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.factorize.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEaMB94vCTr6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1 Loc\n",
        "# Tip: use df['...'].factorize()[0]\n",
        "def parse_labels_to_decimal(df):\n",
        "  #######################################\n",
        "  ########### Your code here ############\n",
        "  #######################################\n",
        "  df['label'] = \n",
        "parse_labels_to_decimal(df)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoRUPnp3hAge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n91lc4cSGx48",
        "colab_type": "text"
      },
      "source": [
        "This builds the labels vocabulary. It shoud return a dictionary with the following format: \n",
        "{..., 1: \"TABLETS\", 2: \"CELLPHONES\", ...}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayHyaOmLGyp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From 1 to 4 LoC.\n",
        "def build_idx_to_label(df):\n",
        "  #######################################\n",
        "  ########### Your code here ############\n",
        "  #######################################\n",
        "  idx_to_label = \n",
        "  return idx_to_label\n",
        "\n",
        "idx_to_label = build_idx_to_label(df)\n",
        "# Check that everything is consistent\n",
        "assert df[df['label']==3].reset_index().iloc[0]['domain_id'] == idx_to_label[3]\n",
        "print(idx_to_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIsh9qdZDvw5",
        "colab_type": "text"
      },
      "source": [
        "We will be using categorical_crossentropy loss function (softmax), for this reason, we need to use one_hot encoding for our labels.\n",
        "\n",
        "The representation should look like: [0, 0, ..., 1, 0] sparse vector with a \"1\" in the position of the target label.\n",
        "  \n",
        " See https://keras.io/utils/ (to_categorial function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAXDqg14DqEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.utils import to_categorical\n",
        "\n",
        "# 1 LoC aprox\n",
        "def vectorize_labels(data, num_labels):\n",
        "  #######################################\n",
        "  ########### Your code here ############\n",
        "  #######################################\n",
        "  # Convert labels to categorical one-hot encoding\n",
        "  # Use to_categorical(... , ...)\n",
        "  one_hot_labels = \n",
        "  return one_hot_labels\n",
        "\n",
        "labels = vectorize_labels(df['label'].values, len(set(df['label'].values)))\n",
        "# Run some Checks \n",
        "assert df.iloc[10]['domain_id'] == idx_to_label[df.iloc[10]['label']]\n",
        "assert labels.shape[1] == num_classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyxz7pr7h5_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(labels[3])\n",
        "df.iloc[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh-dQurRELvA",
        "colab_type": "text"
      },
      "source": [
        "## Split the dataset\n",
        "\n",
        "We will use 80% for training and 20% for validation\n",
        "\n",
        "See: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOy8ONaoEMOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_and_shuffle_dataset(df, val_size=0.2):\n",
        "  # Random shuffle the dataset rows\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  # Split the dataset in train & validation sets\n",
        "  df_train, df_val = train_test_split(df, test_size=val_size)\n",
        "  return df_train.reset_index(drop=True), df_val.reset_index(drop=True)\n",
        "\n",
        "df_train, df_val = split_and_shuffle_dataset(df)\n",
        "print(df_train.shape)\n",
        "print(df_val.shape)\n",
        "df_train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TACn11RcGjE4",
        "colab_type": "text"
      },
      "source": [
        "Vectorize the datasets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMWU8rhQEhDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vectorize_the_datasets(df, df_train, df_val, tokenizer, mode):\n",
        "  x_train_vec = tokenizer.texts_to_matrix(df_train['title'].values, mode)\n",
        "  y_train_vec = vectorize_labels(df_train['label'].values, len(df.domain_id.unique()))\n",
        "\n",
        "  x_val_vec = tokenizer.texts_to_matrix(df_val['title'].values, mode)\n",
        "  y_val_vec = vectorize_labels(df_val['label'].values, len(df.domain_id.unique()))\n",
        "  \n",
        "  return x_train_vec, y_train_vec, x_val_vec, y_val_vec\n",
        "\n",
        "x_train, y_train, x_val, y_val = vectorize_the_datasets(df, df_train, df_val, tokenizer, 'tfidf')  \n",
        "\n",
        "# Run some checks\n",
        "index = 45\n",
        "assert y_train[index][df_train.iloc[index]['label']] == 1.\n",
        "assert y_val[index][df_val.iloc[index]['label']] == 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3R22lkNpjSzz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SImt3ligO7lA",
        "colab_type": "text"
      },
      "source": [
        "# Build the model\n",
        "We will first create a model using Keras Sequential API\n",
        "\n",
        "Learn More: https://keras.io/getting-started/sequential-model-guide/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV2NdFoeHYTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "\n",
        "def build_model(num_features, num_classes):\n",
        "  #######################################\n",
        "  ########### Your code here ############\n",
        "  #######################################\n",
        "  model = # Sequential([...])\n",
        "\n",
        "  # Compile the model\n",
        "  # model.compile(...)\n",
        "  return model\n",
        "\n",
        "model = build_model(x_train.shape[1], y_train.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuXHNIPzPqON",
        "colab_type": "text"
      },
      "source": [
        "### Visualizing the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkqtB3YrPAGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt install graphviz;\n",
        "!pip install pydot pydot-ng;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipBGsMENP5nU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "\n",
        "# Model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Plot model graph\n",
        "plot_model(model, show_shapes=True, show_layer_names=True, to_file='model.png')\n",
        "display(Image('model.png'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hdtUg6sQMfy",
        "colab_type": "text"
      },
      "source": [
        "# Train the model\n",
        "It is time to train the model\n",
        "\n",
        "See: https://keras.io/models/model/ (fit function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl7MlpJtQMOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "def train_model(x_train, y_train, x_val, y_val):\n",
        "  #######################################\n",
        "  ########### Your code here ############\n",
        "  #######################################\n",
        "  # Create callback for early stopping on validation loss. If the loss does\n",
        "  # not decrease in two consecutive tries, stop training.\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=3)]\n",
        "\n",
        "  # Train and validate model.\n",
        "  # history = model.fit(...)\n",
        "  return history\n",
        "\n",
        "history = train_model(x_train, y_train, x_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-E3k3X67SmZX",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYs0ssjJP6eQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_training_history(history):\n",
        "  # Plot training & validation accuracy values\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "visualize_training_history(history)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fPqprqkS9FG",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWIAxB66v5NM",
        "colab_type": "text"
      },
      "source": [
        "Write a function that takes any datasets (train or validation or any other), makes a prediction and return the decimal representation of the label\n",
        "\n",
        "See: model.predict function https://keras.io/models/model/\n",
        "\n",
        "See: np.argmax function https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1sciRRySruZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 LoC aprox.\n",
        "def evaluate_model(x_data, model):\n",
        "  y_pred = model.predict(x_val)\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "  return y_pred\n",
        "\n",
        "y_pred = evaluate_model(x_val, model)\n",
        "y_act = np.argmax(y_val, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOITzf-mwsKO",
        "colab_type": "text"
      },
      "source": [
        "### Visualize the errors and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lueY4Lhyr-wF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
        "from sklearn.metrics import f1_score\n",
        "from pycm import *\n",
        "\n",
        "cm = ConfusionMatrix(actual_vector=y_act, predict_vector=y_pred)\n",
        "cm.save_html(\"cm_html\")\n",
        "#cm.print_matrix()\n",
        "#cm.print_normalized_matrix()\n",
        "#cm.print_matrix(one_vs_all=True, class_name=2)\n",
        "\n",
        "print(\"Global F1-Score:\" , f1_score(y_act, y_pred, average='micro'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppYV7xGsTlYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_errors(df_val, y_act, y_pred, idx_to_label, class_label=None, pred_label=None):\n",
        "  for idx, row in df_val.iterrows():\n",
        "    if class_label is not None and pred_label is not None:\n",
        "      if y_act[idx] != y_pred[idx] and row['label']==class_label and y_pred[idx] == pred_label:\n",
        "        print(row['item_id'], row['title'], 'Truth:', row['domain_id'], 'Prediction:',idx_to_label[y_pred[idx]])\n",
        "    else:\n",
        "      if y_act[idx] != y_pred[idx]:\n",
        "        print(row['item_id'], row['title'], 'Truth:', row['domain_id'], 'Prediction:',idx_to_label[y_pred[idx]])\n",
        "\n",
        "print_errors(df_val, y_act, y_pred, idx_to_label, 17, 8)        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC5bF8AKUHf4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Y_act\", y_act[:10])\n",
        "print(\"Y_pred\",y_pred[:10])\n",
        "print(\"Title:\", df_val.iloc[0]['title'])\n",
        "assert df_val.iloc[1]['domain_id'] == idx_to_label[y_pred[1]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}